{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4f3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/animesh/demo/project/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from data_loader import get_loader\n",
    "\n",
    "# initializing COCO API for instance annotations\n",
    "dataType = 'val2017'\n",
    "dataDir = '/home/animesh/demo/project/opt/cocoapi'\n",
    "instances_annFile = os.path.join(dataDir, 'annotations/instances_{}.json'.format(dataType))\n",
    "captions_annFile = os.path.join(dataDir, 'annotations/captions_{}.json'.format(dataType))\n",
    "coco = COCO(instances_annFile)\n",
    "coco_caps = COCO(captions_annFile)\n",
    "ckeys = coco.anns.keys()\n",
    "ids = list(ckeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060fea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        cnn = models.resnet50(pretrained = True)\n",
    "        \n",
    "        # gradient computation not required since model is pretrained\n",
    "        for param in cnn.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        # list(resnet.children())[:-1] gets a list of all the modules in the ResNet-50 model except for the last one, \n",
    "        # which is the fully connected layer that outputs the class scores. We exclude this layer because we want to \n",
    "        # replace it with our own embedding layer.\n",
    "        \n",
    "        self.cnn = nn.Sequential(*(list(cnn.children())[:-1])) \n",
    "        \n",
    "        # This defines a new linear layer with input size equal to the number of features produced by the ResNet-50 model's \n",
    "        # last fully connected layer (i.e. resnet.fc.in_features) and output size equal to the specified embed_size. \n",
    "        # This linear layer maps the extracted image features to the embedding space.\n",
    "        self.embedding = nn.Linear(cnn.fc.in_features, embed_size)\n",
    "        \n",
    "        \n",
    "    # forward pass of batch of images through Resnet\n",
    "    def forward(self, images):\n",
    "        features = self.cnn(images)\n",
    "        features = features.view(features.size(0),-1) \n",
    "        embedded_features = self.embedding(features)\n",
    "        return embedded_features\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # An embedding layer (nn.Embedding) that maps word indices to embedding vectors.\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # A linear layer (nn.Linear) that maps the LSTM hidden states to word scores, \n",
    "        # i.e., the probability distribution over the vocabulary.\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        # captions[:,:-1] is a slice operation that selects all rows and all columns of the captions tensor, \n",
    "        # except for the last column.This is done to remove the last word index from each caption sequence because \n",
    "        # it is always the <end> token, which is not required as an input to the decoder network during training.\n",
    "        embeddings = self.embed(captions[:,:-1])\n",
    "        \n",
    "        # Here, features.unsqueeze(1) adds an extra dimension to the features tensor at position 1, \n",
    "        # so that its shape becomes (batch_size, 1, feature_size). This is done to make the tensor compatible \n",
    "        # for concatenation with embeddings tensor, which has shape (batch_size, seq_length-1, embed_size).\n",
    "        # The resulting tensor of concatenation has shape (batch_size, seq_length, embed_size) where seq_length \n",
    "        # is the length of the input caption sequence plus one (because we have concatenated the image features \n",
    "        # at the beginning of the sequence).\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        \n",
    "        # an LSTM layer that takes in the concatenated tensor of image features and word embeddings as input.\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        \n",
    "        # The resulting tensor outputs contains the scores of each word in the vocabulary at each time step, indicating \n",
    "        # the probability of the word being the next word in the caption sequence. \n",
    "        outputs = self.linear(hiddens)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150a69e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.03s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                                            | 1004/591753 [00:00<00:58, 10035.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 591753/591753 [00:36<00:00, 16030.26it/s]\n",
      "/home/animesh/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/animesh/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "print_every = 100          # determines window for printing average loss\n",
    "\n",
    "norm1 = (0.485, 0.456, 0.406)\n",
    "norm2 = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.RandomVerticalFlip(0.1),\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize(norm1,norm2      # normalize image for pre-trained model\n",
    "                         )])\n",
    "\n",
    "# Building data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "datas = data_loader.dataset\n",
    "vocab_size = len(datas.vocab)\n",
    "\n",
    "# Initializing the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Defining the loss function. \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embedding.parameters()) \n",
    "\n",
    "#optimizer.\n",
    "optimizer = torch.optim.Adam(params=params, lr = 0.001)\n",
    "\n",
    "# The total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb02c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [100/4624], Loss: 3.7198, Perplexity: 41.25523\n",
      "Epoch [5/5], Step [200/4624], Loss: 3.3102, Perplexity: 27.39091\n",
      "Epoch [5/5], Step [300/4624], Loss: 3.1293, Perplexity: 22.8578\n",
      "Epoch [5/5], Step [400/4624], Loss: 3.0080, Perplexity: 20.24750\n",
      "Epoch [5/5], Step [500/4624], Loss: 3.8884, Perplexity: 48.8309\n",
      "Epoch [5/5], Step [600/4624], Loss: 3.0953, Perplexity: 22.0935\n",
      "Epoch [5/5], Step [700/4624], Loss: 2.9158, Perplexity: 18.4628\n",
      "Epoch [5/5], Step [800/4624], Loss: 2.7595, Perplexity: 15.7921\n",
      "Epoch [5/5], Step [900/4624], Loss: 2.6937, Perplexity: 14.7869\n",
      "Epoch [5/5], Step [1000/4624], Loss: 2.4682, Perplexity: 11.8010\n",
      "Epoch [5/5], Step [1100/4624], Loss: 3.1929, Perplexity: 24.3601\n",
      "Epoch [5/5], Step [1200/4624], Loss: 2.3458, Perplexity: 10.4418\n",
      "Epoch [5/5], Step [1300/4624], Loss: 2.3857, Perplexity: 10.8669\n",
      "Epoch [5/5], Step [1400/4624], Loss: 2.4690, Perplexity: 11.8107\n",
      "Epoch [5/5], Step [1500/4624], Loss: 2.4626, Perplexity: 11.7348\n",
      "Epoch [5/5], Step [1600/4624], Loss: 2.5310, Perplexity: 12.5656\n",
      "Epoch [5/5], Step [1700/4624], Loss: 2.4698, Perplexity: 11.8201\n",
      "Epoch [5/5], Step [1800/4624], Loss: 2.3490, Perplexity: 10.4747\n",
      "Epoch [5/5], Step [1900/4624], Loss: 2.3237, Perplexity: 10.21345\n",
      "Epoch [5/5], Step [2000/4624], Loss: 2.3106, Perplexity: 10.0800\n",
      "Epoch [5/5], Step [2100/4624], Loss: 2.2525, Perplexity: 9.51113\n",
      "Epoch [5/5], Step [2200/4624], Loss: 2.9134, Perplexity: 18.4202\n",
      "Epoch [5/5], Step [2300/4624], Loss: 2.2827, Perplexity: 9.80270\n",
      "Epoch [5/5], Step [2400/4624], Loss: 2.3842, Perplexity: 10.8502\n",
      "Epoch [5/5], Step [2450/4624], Loss: 2.2959, Perplexity: 9.93315"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sampling a caption length, and sample indices with that length.\n",
    "        datas = data_loader.dataset \n",
    "        indices = datas.get_train_indices()\n",
    "        # Creating and assigning a batch sampler to retrieve a batch with the sampled indices.\n",
    "        data_s = data.sampler\n",
    "        new_sampler = data_s.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        images, captions = next(iter(data_loader))\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        # Passing the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        # Calculating the batch loss.\n",
    "        view_outputs = outputs.view(-1, vocab_size)\n",
    "        view = captions.view(-1)\n",
    "        loss = criterion(view_outputs,view)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d],Perplexity: %5.4f ,Loss: %.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Saving the weights.\n",
    "    if epoch:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "        torch.save(optimizer.state_dict(), os.path.join('./models', 'optim-%d.pkl' % epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a5386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
