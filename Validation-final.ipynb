{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4f3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/animesh/Documents/project/opt/cocoapi/PythonAPI')\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from data_loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060fea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        cnn = models.resnet50(pretrained = True)\n",
    "        \n",
    "        # gradient computation not required since model is pretrained\n",
    "        for param in cnn.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        # list(resnet.children())[:-1] gets a list of all the modules in the ResNet-50 model except for the last one, \n",
    "        # which is the fully connected layer that outputs the class scores. We exclude this layer because we want to \n",
    "        # replace it with our own embedding layer.\n",
    "        \n",
    "        self.cnn = nn.Sequential(*(list(cnn.children())[:-1])) \n",
    "        \n",
    "        # This defines a new linear layer with input size equal to the number of features produced by the ResNet-50 model's \n",
    "        # last fully connected layer (i.e. resnet.fc.in_features) and output size equal to the specified embed_size. \n",
    "        # This linear layer maps the extracted image features to the embedding space.\n",
    "        self.embedding = nn.Linear(cnn.fc.in_features, embed_size)\n",
    "        \n",
    "        \n",
    "    # forward pass of batch of images through Resnet\n",
    "    def forward(self, images):\n",
    "        features = self.cnn(images)\n",
    "        features = features.view(features.size(0),-1) \n",
    "        embedded_features = self.embedding(features)\n",
    "        return embedded_features\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # An embedding layer (nn.Embedding) that maps word indices to embedding vectors.\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # A linear layer (nn.Linear) that maps the LSTM hidden states to word scores, \n",
    "        # i.e., the probability distribution over the vocabulary.\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        # captions[:,:-1] is a slice operation that selects all rows and all columns of the captions tensor, \n",
    "        # except for the last column.This is done to remove the last word index from each caption sequence because \n",
    "        # it is always the <end> token, which is not required as an input to the decoder network during training.\n",
    "        embeddings = self.embed(captions[:,:-1])\n",
    "        \n",
    "        # Here, features.unsqueeze(1) adds an extra dimension to the features tensor at position 1, \n",
    "        # so that its shape becomes (batch_size, 1, feature_size). This is done to make the tensor compatible \n",
    "        # for concatenation with embeddings tensor, which has shape (batch_size, seq_length-1, embed_size).\n",
    "        # The resulting tensor of concatenation has shape (batch_size, seq_length, embed_size) where seq_length \n",
    "        # is the length of the input caption sequence plus one (because we have concatenated the image features \n",
    "        # at the beginning of the sequence).\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        \n",
    "        # an LSTM layer that takes in the concatenated tensor of image features and word embeddings as input.\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        \n",
    "        # The resulting tensor outputs contains the scores of each word in the vocabulary at each time step, indicating \n",
    "        # the probability of the word being the next word in the caption sequence. \n",
    "        outputs = self.linear(hiddens)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size),\n",
    "                torch.zeros(1, batch_size, self.hidden_size))\n",
    "    \n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
    "        \n",
    "        # Initialize the hidden state\n",
    "        hidden = self.init_hidden(inputs.shape[0])# features is of shape (batch_size, embed_size)\n",
    "        \n",
    "        out_list = list()\n",
    "        word_len = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            while word_len < max_len:\n",
    "                lstm_out, hidden = self.lstm(inputs, hidden)\n",
    "                out = self.linear(lstm_out)\n",
    "                out = out.squeeze(1)\n",
    "                out = out.argmax(dim=1)\n",
    "                # print(out)\n",
    "                out_list.append(out.item())\n",
    "                \n",
    "                inputs = self.embed(out.unsqueeze(0))\n",
    "                \n",
    "                word_len += 1\n",
    "                if out == 1:\n",
    "                    break\n",
    "        \n",
    "        return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05307874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a transform to pre-process the testing images.\n",
    "\n",
    "norm1 = (0.485, 0.456, 0.406)\n",
    "norm2 = (0.229, 0.224, 0.225)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm1,norm2)\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65c9b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 591753/591753 [00:49<00:00, 12031.32it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True \n",
    "\n",
    "# data loader.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.RandomVerticalFlip(0.1),\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize(norm1,      # normalize image for pre-trained model\n",
    "                         norm2)])\n",
    "\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de38578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/animesh/demo/notebookenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/animesh/demo/notebookenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_file = 'encoder-5.pkl'\n",
    "decoder_file = 'decoder-5.pkl'\n",
    "\n",
    "embed_size = 512\n",
    "hidden_size = 512\n",
    "\n",
    "# The size of the vocabulary.\n",
    "datas = data_loader.dataset\n",
    "vocab_size = len(datas.vocab)\n",
    "\n",
    "# Initializing the encoder and decoder, and set each to inference mode.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "encoder.eval()\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "decoder.eval()\n",
    "\n",
    "# Loading the trained weights.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646a2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(output):\n",
    "    parts = [data_loader.dataset.vocab.idx2word[i] for i in output][1:-1]\n",
    "    sentence = \" \".join(parts)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07df153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 25014/25014 [00:02<00:00, 8626.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/animesh/Documents/project/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loaderval import get_loaderval\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm1,\n",
    "                         norm2)\n",
    "])\n",
    "\n",
    "# Creating the data loader.\n",
    "data_loader = get_loaderval(transform=transform_test,    \n",
    "                         mode='validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5137197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500 images\n",
      "processed 500 images\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open the output file for writing\n",
    "with open(\"captions_val2017_results.json\", \"w\") as f:\n",
    "    #f.write('[')\n",
    "    i = 0\n",
    "    arr = []\n",
    "    notarr=[]\n",
    "    unique = set()\n",
    "    while True:\n",
    "        try:\n",
    "            image_id, image = next(iter(data_loader))\n",
    "            i = i + 1\n",
    "            if i%50==0:\n",
    "                print(\"processed 500 images\")\n",
    "            if i % 100 == 0 :\n",
    "                break\n",
    "\n",
    "            features = encoder(image).unsqueeze(1)\n",
    "            output = decoder.sample(features)    \n",
    "            sentence = clean_sentence(output)\n",
    "\n",
    "            item = {\"image_id\": int(image_id[0]), \"caption\": sentence}\n",
    "            if int(image_id[0]) not in unique:\n",
    "                arr.append(item.copy())\n",
    "            unique.add(int(image_id[0]))\n",
    "            notarr.append(item.copy())\n",
    "\n",
    "        except StopIteration:\n",
    "            # no more data, exit the loop\n",
    "            print(\"breaking out..\")\n",
    "            print(\"Processed all validation images\")\n",
    "            break\n",
    "    json.dump(arr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce629e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
